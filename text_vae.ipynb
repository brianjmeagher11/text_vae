{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6690cc8",
   "metadata": {},
   "source": [
    "# VAE for text generation\n",
    "\n",
    "In this notebook, I aim to create a VAE that can generate synthetic text data. My initial use case is to create synthetic tweets, but I may play with other types of training data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15fded35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d2fb5",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec6679",
   "metadata": {},
   "source": [
    "Get the data we want, then set up a dataset and dataloader for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bb0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before anything else, grab a tokenizer.\n",
    "tokenizer_name = \"bert-base-uncased\" # Using a recommended tokenizer I saw suggested online.\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d8bf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n"
     ]
    }
   ],
   "source": [
    "# Get the kaggle dataset we want.\n",
    "path = kagglehub.dataset_download('goyaladi/twitter-dataset')\n",
    "df = pd.read_csv(f\"{path}/twitter_dataset.csv\")\n",
    "df.head()\n",
    "\n",
    "# Define the dataset class. We need a length and getitem method in addition to the initializer.\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.text = df.Text.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # We need to return both the padded token sets and their original lengths\n",
    "        # Get the token IDs, and squeeze to get rid of the batch dimension.\n",
    "        tokens = self.tokenizer(\n",
    "            self.text[idx],\n",
    "            return_tensors='pt',\n",
    "        ).input_ids.squeeze(0)\n",
    "\n",
    "        # Get the lengths, store as tensor to keep consistent with token types.\n",
    "        length = torch.tensor(len(tokens))\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokens,\n",
    "            \"lengths\": length\n",
    "        }\n",
    "\n",
    "# Create the dataset.\n",
    "dataset = TextDataset(df, tokenizer)\n",
    "\n",
    "# Now create a collation function to handle unequal lengths.\n",
    "def collate_fn(batch):\n",
    "    # Pad the tokens\n",
    "    tokens = [item['input_ids'] for item in batch]\n",
    "    padded_tokens = nn.utils.rnn.pad_sequence(tokens, batch_first=True)\n",
    "    \n",
    "    # Get the lengths, and store as tensor\n",
    "    lengths = [item['lengths'] for item in batch]\n",
    "    lengths = torch.stack(lengths)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": padded_tokens,\n",
    "        \"lengths\": lengths\n",
    "    }\n",
    "\n",
    "# Now create the dataloader.\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shining dataset as a test. \"All work and no play makes Jack a dull boy.\"\n",
    "phrase = \"All work and no play makes Jack a dull boy.\"\n",
    "n = 10000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Text': [phrase] * n\n",
    "})\n",
    "\n",
    "class ShiningDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.length = len(self.df)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenizer(\n",
    "            self.df.iloc[idx]['Text'],\n",
    "            return_tensors = 'pt'\n",
    "        )['input_ids'].squeeze(0)\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'lengths': input_ids.shape[0]\n",
    "        }\n",
    "    \n",
    "dataset = ShiningDataset(df, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb42c3",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90343a8",
   "metadata": {},
   "source": [
    "The model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487fc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVAE(nn.Module):\n",
    "    # A VAE that recreates text.\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            hidden_dim,        \n",
    "            num_layers,\n",
    "            dropout,\n",
    "            latent_dim,\n",
    "            teacher_forcing_ratio=0.0\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the VAE.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        vocab_size: Size of the model vocabulary. Recommend to set equal \n",
    "                    to the number of unique tokens in the training data.\n",
    "        embedding_dim: Dimension of the space in which tokens are embedded.\n",
    "        hidden_dim: Dimension of the hidden layer between the GRU and the\n",
    "                    latent space.\n",
    "        num_layers: Number of layers in each GRU.\n",
    "        dropout: Dropout probability for the GRU.\n",
    "        latent_dim: Dimension of the latent space\n",
    "        teacher_forcing_ratio: Teacher forcing ratio, proportion of true labels\n",
    "                               passed directly to model during decoding.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the superclass.\n",
    "        super().__init__()\n",
    "\n",
    "        # Store latent_dim for later use.\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        # Embedding layer: Convert tokens into vectors. Used in both encoder and decoder.\n",
    "        # ----\n",
    "        # Input: tensor (batch_size, seq_length)\n",
    "        # Output: tensor (batch_size, seq_length, embedding_dim)\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        # ----\n",
    "\n",
    "        # Encoder GRU: Reads tokens into GRU, a recurrent module which can handle the\n",
    "        # sequence of tokens. Since we're using the hidden states, be mindful to transpose\n",
    "        # things so that the tensor has the right shape afterwards!\n",
    "        # Input: tensor (batch_size, seq_length, embedding_dim)\n",
    "        # Output: output tensor (batch_size, seq_length, hidden_dim)\n",
    "        #         hidden state tensor (num_layers, batch_size, hidden_dim)\n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        # ----\n",
    "\n",
    "        # Fully connected layers: One each for mu and log variance. Note that \n",
    "        # this only uses the hidden state of the last token in the sequence! \n",
    "        # The relevant dimensional shuffling is done in the encoding function. \n",
    "        # In both cases:\n",
    "        # Input: tensor (batch_size, hidden_dim)\n",
    "        # Output: tensor (batch_size, latent_dim)\n",
    "        self.fc_mu = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=latent_dim,\n",
    "        )\n",
    "        self.fc_logvar = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=latent_dim,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        # ----\n",
    "        \n",
    "        # Fully connected layer: Going from the latent space to the decoder's GRU.\n",
    "        # Input: tensor (batch_size, seq_length, latent_dim)\n",
    "        # Output: tensor (batch_size, seq_length, hidden_dim)\n",
    "        self.fc_decoder_init = nn.Linear(\n",
    "            in_features=latent_dim,\n",
    "            out_features=hidden_dim\n",
    "        )\n",
    "\n",
    "        # Decoder GRU: The recurrent part of the decoder. Just like the encoder!\n",
    "        # Input: tensor (batch_size, seq_length, embedding_dim)\n",
    "        # Output: output tensor (batch_size, seq_length, hidden_dim)\n",
    "        #         hidden state tensor (num_layers, batch_size, hidden_dim)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Output layer: Convert GRU hidden states into token logits. Note that \n",
    "        # this only uses the hidden state of the last token in the sequence! \n",
    "        # The relevant dimensional shuffling is done in the decoding function.\n",
    "        # Input: tensor (batch_size, hidden_dim)\n",
    "        # Output: tensor (batch_size, vocab_size)\n",
    "        self.fc_decoder_output = nn.Linear(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "    \n",
    "    def encode(\n",
    "            self, \n",
    "            x, \n",
    "            lengths\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Encode input sequences into a latent distribution, i.e. mu and log variance.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        x: Input tensor of token indices. Shape: (batch_size, seq_length)\n",
    "        lengths: Actual lengths of the sequences before padding. Shape: (batch_size)\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        mu, logvar: Tensors representing the parameters of the latent distribution. Shape: (batch_size, latent_dim) \n",
    "        \"\"\"\n",
    "\n",
    "        # Embed the sequences.\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "\n",
    "        # Pack the sequences for GRU processing. This is a PyTorch data structure that\n",
    "        # helps the GRU ignore padding tokens and process things faster.\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            input=embedded,\n",
    "            lengths=lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # Encode the packed sequence through the GRU.\n",
    "        _, hidden = self.encoder_gru(packed)\n",
    "\n",
    "        # Get the hidden state of the last layer.\n",
    "        last_hidden = hidden[-1:].squeeze(0).contiguous()\n",
    "        last_hidden = last_hidden.view(last_hidden.size(0), -1)\n",
    "\n",
    "        # Get the mean and variance.\n",
    "        mu = self.fc_mu(last_hidden)\n",
    "        logvar = self.fc_logvar(last_hidden)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def latent_sample(\n",
    "            self, \n",
    "            mu, \n",
    "            logvar\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Sample from the latent distribution using the reparameterization trick.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        mu: Mean tensor of the latent distribution. Shape: (batch_size, latent_dim)\n",
    "        logvar: Log variance tensor of the latent distribution. Shape: (batch_size, latent_dim)\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        z: Sampled latent tensor. Shape: (batch_size, latent_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the standard deviation.\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "\n",
    "        # Add Gaussian noise.\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        # Now return the latent vector.\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def decode(\n",
    "            self, \n",
    "            z, \n",
    "            max_length,\n",
    "            target_tokens,\n",
    "            teacher_forcing_ratio\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Decode latent vectors into output token logits.\n",
    "        \n",
    "        Inputs:\n",
    "        ----\n",
    "        z: Latent tensor. Shape: (batch_size, latent_dim)\n",
    "        max_length: Maximum length of the output sequences to generate.\n",
    "        target_tokens: Set of actual tokens\n",
    "        teacher_forcing_ratio: How often decoder should be given true tokens\n",
    "                               rather than its own predictions.\n",
    "        \n",
    "        Outputs:\n",
    "        ----\n",
    "        logits: Output token logits: Shape: (batch_size, max_length, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the batch size from the latent tensor.\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        # Get the initial hidden state for the decoder GRU.\n",
    "        decoder_hidden = self.fc_decoder_init(z)\n",
    "\n",
    "        # Shuffle some dimensions around so this is in the proper shape. Indexed by\n",
    "        # (num_layers, batch_size, hidden_dim)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "\n",
    "        # Prepare the input tokens for the decoder GRU. Start with all start of\n",
    "        # sequence tokens.\n",
    "        current_input = torch.full(\n",
    "            size=(batch_size, 1), \n",
    "            fill_value=1, \n",
    "            dtype=torch.long,\n",
    "            device=z.device\n",
    "        )\n",
    "\n",
    "        # Prepare a list to collect the logits.\n",
    "        logits = []\n",
    "\n",
    "        # Produce a logit for each possible max token.\n",
    "        for i in range(max_length):\n",
    "\n",
    "            # Embed the current inputs.\n",
    "            embedded = self.embedding(current_input)\n",
    "\n",
    "            # Decode the embedding through the GRU. The old value\n",
    "            # of decoder_hidden is used to produce the new one.\n",
    "            output, decoder_hidden = self.decoder_gru(embedded, decoder_hidden)\n",
    "\n",
    "            # Send the output through to get the logits. Squeeze the 1st\n",
    "            # dimension (the seq_length), since we're processing one token\n",
    "            # at a time, and need to match the expected input shape of the \n",
    "            # fully connected layer.\n",
    "            out = self.fc_decoder_output(output.squeeze(1))\n",
    "            logits.append(out)\n",
    "\n",
    "            # Get the next input tokens by grabbing the argmax of the logits,\n",
    "            # or grab the next real token if teacher forcing is in play.\n",
    "            # TODO: Fix indexing, currently will go out of bounds on shorter sequences\n",
    "            if np.random.rand() < teacher_forcing_ratio:\n",
    "                current_input = torch.argmax(out, dim=-1).unsqueeze(1)\n",
    "            else:\n",
    "                current_input = target_tokens[i]\n",
    "\n",
    "        # Finally, stack the logits and return them. Stack vs. cat here, since\n",
    "        # we're adding the sequence length back in as a new dimension.\n",
    "        logits = torch.stack(logits, dim=1)\n",
    "        return logits\n",
    "        \n",
    "    def forward(self, x, lengths, max_length=None):\n",
    "        \"\"\"\n",
    "        The network's forward pass. Returns both the reconstructed logits and\n",
    "        the mu and logvar parameters of the latent distribution. We need both\n",
    "        to calculate the loss.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        x: Input sequences. Shape: (batch_size, seq_length)\n",
    "        lengths: Actual lengths of each sequence.\n",
    "        max_lengths: Maximum length for decoder, defaults to input length\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        logits: Logits of the reconstruted sequences. \n",
    "        Shape: (batch_size, max_length, vocab_size)\n",
    "        mu: Mean vectors of the latent distributions. Shape: (batch_size, \n",
    "        latent_dim).\n",
    "        logvar: Log variance vectors of the latent distributions. Shape: \n",
    "        (batch_size, latent_dim).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set the default max_length.\n",
    "        if max_length is None:\n",
    "            max_length = x.size(1)\n",
    "        \n",
    "        # Encode the input sequences.\n",
    "        mu, logvar = self.encode(x, lengths)\n",
    "\n",
    "        # Sample from the latent distribution.\n",
    "        z = self.latent_sample(mu, logvar)\n",
    "\n",
    "        # Now decode the latent vectors.\n",
    "        logits = self.decode(z, max_length)\n",
    "\n",
    "        return logits, mu, logvar\n",
    "    \n",
    "    def generate(self, num_samples, max_length, mu = None, logvar = None):\n",
    "        \"\"\"\n",
    "        Generate samples from the latent space.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        num_samples: Number of samples to generate.\n",
    "        max_length: Maximum length of each sample, in tokens.\n",
    "        mu: optional mean tensor for the latent distribution. If None, \n",
    "            samples will be drawn from a standard normal distribution.\n",
    "        logvar: optional log variance tensor for the latent distribution.\n",
    "                If None, samples will be drawn from a standard normal\n",
    "                distribution.\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        tokens: Generated samples' token indices. Shape: (num_samples, max_length)\n",
    "        \"\"\"\n",
    "        # Set default mu and logvar values, if necessary.\n",
    "        if mu is None:\n",
    "            mu = torch.zeros((1, self.latent_dim), device=next(self.parameters()).device)\n",
    "        if logvar is None:\n",
    "            logvar = torch.zeros((1, self.latent_dim), device=next(self.parameters()).device)\n",
    "\n",
    "        # Expand mu and logvar to match num_samples. Shape will\n",
    "        # be (num_samples, latent_dim)\n",
    "        mu = mu.expand(num_samples, -1)\n",
    "        logvar = logvar.expand(num_samples, -1)\n",
    "\n",
    "        # Set into eval mode, and disable gradient calculations.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Sample from the distribution.\n",
    "            z = self.latent_sample(mu, logvar)\n",
    "\n",
    "            # Now decode the latent vectors and get the logits. Shape\n",
    "            # is \n",
    "            logits = self.decode(z, max_length)\n",
    "\n",
    "            # Get the token indices by grabbing the largest logits' \n",
    "            # indices. Do across the last dimension, i.e. the vocab\n",
    "            # dimension.\n",
    "            tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758f491",
   "metadata": {},
   "source": [
    "Now the loss function, which incorporates both reconstruction loss and KL-divergence from the prior distribution we want. The loss function is really the evidence lower bound (ELBO) of variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23955a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(\n",
    "    recon_logits,\n",
    "    target_tokens,\n",
    "    mu,\n",
    "    logvar,\n",
    "    prior_mu=None,\n",
    "    prior_logvar=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the VAE's loss, combining reconstruction loss and KL\n",
    "    divergence from a prior distribution.\n",
    "\n",
    "    Inputs:\n",
    "    ----\n",
    "    recon_logits: Reconstructed logits from the VAE. Shape: (batch_size, seq_length, vocab_size)\n",
    "    target_tokens: Target token indices, the ones we want the logits to be\n",
    "                   close to. Shape: (batch_size, seq_length)\n",
    "    mu: Mean vectors of the latent distributions. Shape: (batch_size, latent_dim)\n",
    "    logvar: Log variance vectors of the latent distributions. Shape: (batch_size, latent_dim)\n",
    "    prior_mu: Mean of the prior distribution, default unit normal. Shape: (latent_dim)\n",
    "    prior_logvar: Log variance of the prior distribution, default unit normal.\n",
    "                  Shape: (latent_dim)\n",
    "    \n",
    "    Outputs:\n",
    "    ----\n",
    "    loss: The VAE loss.\n",
    "    recon_loss: The reconstruction part of the loss.\n",
    "    kl_div: The KL divergence part of the loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, get the shape of the reconstructed logits so we can get\n",
    "    # the batch size, sequence length, and vocabulary size.\n",
    "    batch_size, seq_length, vocab_size = recon_logits.shape\n",
    "    latent_dim = mu.shape[1]\n",
    "\n",
    "    # Now set up the prior distribution parameters, if needed.\n",
    "    # TODO: Add logvar\n",
    "    if prior_mu is None:\n",
    "        prior_mu = torch.zeros((1, latent_dim), device=recon_logits.device)\n",
    "    if prior_logvar is None:\n",
    "        prior_logvar = torch.zeros((1, latent_dim), device=recon_logits.device)\n",
    "\n",
    "    # Align the target tokens with the reconstructed logits. The first reconstructed logits\n",
    "    # correspond to the model's predictions after it has *seen* the first token, not for the\n",
    "    # first token itself! Likewise, we don't care about the last set of logits; they correspond\n",
    "    # to the predictions for the token after the last token in the sequence, i.e. a token which\n",
    "    # does not actually exist!\n",
    "    #\n",
    "    # As such, chop off the last logits, as well as the first token. Then make everything\n",
    "    # contiguous in memory so it plays nice with .view() in the next step.\n",
    "    recon_logits = recon_logits[:, :-1, :].contiguous()\n",
    "    target_tokens = target_tokens[:, 1:].contiguous()\n",
    "\n",
    "    # Now we flatten the logits and target tokens. Remember, at this\n",
    "    # point, we don't care about keeping the sequences separate; we\n",
    "    # just need to know how close each logit is to its target token.\n",
    "    logits_flat = recon_logits.view(-1, vocab_size) # One value per possible token, per token.\n",
    "    targets_flat = target_tokens.view(-1) # Just a flat list of target tokens.\n",
    "\n",
    "    # Calculate the reconstruction loss as cross-entropy.\n",
    "    recon_loss = cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "    # Now calculate the KL divergence. This uses a closed-form solution\n",
    "    # for the KL divergence between two Gaussians.\n",
    "    kl_div = 0.5 * torch.sum(\n",
    "        torch.exp(logvar - prior_logvar) + (prior_mu - mu) ** 2\n",
    "        / torch.exp(prior_logvar) - logvar + prior_logvar - 1\n",
    "    )\n",
    "\n",
    "    # Finally, return the completed loss.\n",
    "    loss = recon_loss + kl_div\n",
    "\n",
    "    return loss, recon_loss, kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d67c61",
   "metadata": {},
   "source": [
    "Now define the training loop we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12cbf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    prior_mu=None,\n",
    "    prior_logvar=None,\n",
    "    epochs=1,\n",
    "    print_freq=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a VAE on a set of training data.\n",
    "\n",
    "    Inputs:\n",
    "    ----\n",
    "    model: The VAE to be trained.\n",
    "    train_loader: The dataloader for our training dataset.\n",
    "    optimizer: The optimizer we're using.\n",
    "    prior_mu: Mean of the prior distribution. Default unit normal.\n",
    "    prior_logvar: Log variance of the prior distribution. Default unit normal.\n",
    "    epochs: Number of epochs to train for. Default 1.\n",
    "    print_freq: Frequency of printing training progress, in batches. Default 50.\n",
    "\n",
    "    Outputs:\n",
    "    ----\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab the device the model is on.\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Set the prior distribution parameters, if needed. We'll need to expand these later\n",
    "    # to match the batch sizes. We can't do this in advance, since the batches may vary.\n",
    "    if prior_mu is None:\n",
    "        prior_mu = torch.zeros(\n",
    "            size = (1, model.latent_dim),\n",
    "            device = device,\n",
    "        )\n",
    "    if prior_logvar is None:\n",
    "        prior_logvar = torch.zeros(\n",
    "            size = (1, model.latent_dim),\n",
    "            device = device\n",
    "        )\n",
    "\n",
    "    # Guardian statements to ensure prior tensors have correct shape:\n",
    "    assert prior_mu.shape == (1, model.latent_dim), \"prior_mu must have shape (1, latent_dim)\"\n",
    "    assert prior_logvar.shape == (1, model.latent_dim), \"prior_logvar must have shape (1, latent_dim)\"\n",
    "\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over epochs.\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Iterate over batches in the data loader.\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Get the tweets and their lengths.\n",
    "            tokens = batch['input_ids']\n",
    "            lengths = batch['lengths']\n",
    "\n",
    "            # Get the batch size.\n",
    "            batch_size = tokens.size(0)\n",
    "\n",
    "            # Zero the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Run the forward pass through the model.\n",
    "            recon_logits, mu, logvar = model(tokens, lengths)\n",
    "\n",
    "            # Calculate the loss, expanding the prior parameters to match the batch size.\n",
    "            loss, recon_loss, kl_div = vae_loss(\n",
    "                recon_logits,\n",
    "                tokens,\n",
    "                mu,\n",
    "                logvar,\n",
    "                prior_mu=prior_mu.expand(batch_size, -1),\n",
    "                prior_logvar=prior_logvar.expand(batch_size, -1)\n",
    "            )\n",
    "\n",
    "            # Backpropagate the loss.\n",
    "            loss.backward()\n",
    "\n",
    "            # Step the optimizer to update the parameters of the model.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print progress when needed.\n",
    "            if (batch_idx+1) % print_freq == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, batch {batch_idx+1}/{len(train_loader)}. Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a00a0c",
   "metadata": {},
   "source": [
    "# The training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32eba7e",
   "metadata": {},
   "source": [
    "The actual training at long last!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8af0401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set important model parameters.\n",
    "vocab_size = tokenizer.vocab_size # Use the tokenizer's vocabulary size.\n",
    "embedding_dim = 128 # Small-ish embedding dimension.\n",
    "hidden_dim = 256 # Small-to-middling hidden dimension for the GRU.\n",
    "num_layers = 1 # Number of GRU layers, set to just one.\n",
    "dropout = 0. # No dropout for now. Probably a bad idea, but let's see how it does.\n",
    "latent_dim = 32 # Dimension of the latent space. Small in general, large compared to what I've seen!\n",
    "lr = 1e-3 # Learning rate, a common value used.\n",
    "\n",
    "# Instantiate the model.\n",
    "textVAE = TextVAE(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    latent_dim=latent_dim\n",
    ")\n",
    "\n",
    "# Place on device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = Adam(textVAE.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3db164ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, batch 50/313. Loss: 5.4127\n",
      "Epoch 1/1, batch 100/313. Loss: 5.1583\n",
      "Epoch 1/1, batch 150/313. Loss: 5.0883\n",
      "Epoch 1/1, batch 200/313. Loss: 5.0935\n",
      "Epoch 1/1, batch 250/313. Loss: 4.9026\n",
      "Epoch 1/1, batch 300/313. Loss: 5.1027\n"
     ]
    }
   ],
   "source": [
    "# At long last, train the model!\n",
    "train_vae(\n",
    "    model=textVAE,\n",
    "    train_loader=dataloader,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d06ac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: ......................................\n",
      "Sample 1: .....................................\n",
      "Sample 2: .....................................\n",
      "Sample 3: ....................................\n",
      "Sample 4: .....................................\n"
     ]
    }
   ],
   "source": [
    "# Generate a few examples.\n",
    "num_samples = 5\n",
    "max_length = 50\n",
    "\n",
    "tokens = textVAE.generate(\n",
    "    num_samples = num_samples,\n",
    "    max_length = max_length\n",
    ")\n",
    "\n",
    "text = tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "for i, t in enumerate(text):\n",
    "    print(f\"Sample {i}: {t}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
