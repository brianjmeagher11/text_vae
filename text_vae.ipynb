{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6690cc8",
   "metadata": {},
   "source": [
    "# VAE for text generation\n",
    "\n",
    "Just a little experiment I'm running to see if I can do it. I have an agentic version of this repo too, but want to make sure I can code it from scratch as well.\n",
    "\n",
    "My goal is to get a network that can generate synthetic tweets after being trained on an appropriately sized dataset... or at least could, in principle. I'll need to learn more about how to set up recurrent architectures than I currently know, so this is a good exercise for me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fded35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVAE(nn.Module):\n",
    "    # A VAE that recreates text. Specifically, I'll use it for tweets.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "        latent_dim\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the VAE.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        vocab_size: Size of the model vocabulary. Recommend to set equal \n",
    "                    to the number of unique tokens in the training data.\n",
    "        embedding_dim: Dimension of the space in which words are embedded.\n",
    "        hidden_dim: Dimension of the hidden layer between the GRU and the\n",
    "                    latent space.\n",
    "        num_layers: Number of layers in each GRU.\n",
    "        dropout: Dropout probability for the GRU.\n",
    "        latent_dim: The dimension of the latent space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        # ----\n",
    "\n",
    "        # The embedding layer. Here, words are translated to vectors for use\n",
    "        # in the model. padding_idx here just specifies what value counts as\n",
    "        # padding. \n",
    "        # Input: tensor (seq_length)\n",
    "        # Output: tensor (seq_length, embedding_dim)\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        # GRU, the recurrent part of the encoder. This allows us to use text \n",
    "        # data with the VAE, since recurrent architectures are well-suited to \n",
    "        # such things. \n",
    "        # Input: tensor (seq_length, embedding_dim)\n",
    "        # Output: tensor (seq_length, hidden_dim)\n",
    "        self.encoder_rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        # ----\n",
    "\n",
    "        # Fully connected layers for mu and log variance. In both cases:\n",
    "        # Input: tensor (seq_length, hidden_dim)\n",
    "        # Output: tensor (seq_length, latent_dim)\n",
    "        self.fc_mu = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=latent_dim,\n",
    "        )\n",
    "        self.fc_logvar = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=latent_dim,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        # ----\n",
    "        \n",
    "        # Fully connected layer to go from latent space to decoder GRU.\n",
    "        # Input: tensor (seq_length, latent_dim)\n",
    "        # Output: tensor (seq_length, hidden_dim)\n",
    "        self.fc_decoder_init = nn.Linear(\n",
    "            in_features=latent_dim,\n",
    "            out_features=hidden_dim\n",
    "        )\n",
    "\n",
    "        # GRU, the recurrent part of the decoder. Just like before!\n",
    "        # Input: tensor (seq_length, embedding_dim)\n",
    "        # Output: tensor (seq_length, hidden_dim)\n",
    "        self.decoder_rnn = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
