{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6690cc8",
   "metadata": {},
   "source": [
    "# VAE for text generation\n",
    "\n",
    "In this notebook, I aim to create a VAE that can generate synthetic text data. My initial use case is to create synthetic tweets, but I may play with other types of training data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15fded35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d2fb5",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec6679",
   "metadata": {},
   "source": [
    "Get the data we want, then set up a dataset and dataloader for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before anything else, grab a tokenizer.\n",
    "tokenizer_name = \"bert-base-uncased\" # Using a recommended tokenizer I saw suggested online.\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8bf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>julie81</td>\n",
       "      <td>Party least receive say or single. Prevent pre...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2023-01-30 11:00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>richardhester</td>\n",
       "      <td>Hotel still Congress may member staff. Media d...</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>2023-01-02 22:45:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>williamsjoseph</td>\n",
       "      <td>Nice be her debate industry that year. Film wh...</td>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>2023-01-18 11:25:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>danielsmary</td>\n",
       "      <td>Laugh explain situation career occur serious. ...</td>\n",
       "      <td>37</td>\n",
       "      <td>18</td>\n",
       "      <td>2023-04-10 22:06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>carlwarren</td>\n",
       "      <td>Involve sense former often approach government...</td>\n",
       "      <td>27</td>\n",
       "      <td>80</td>\n",
       "      <td>2023-01-24 07:12:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_ID        Username  \\\n",
       "0         1         julie81   \n",
       "1         2   richardhester   \n",
       "2         3  williamsjoseph   \n",
       "3         4     danielsmary   \n",
       "4         5      carlwarren   \n",
       "\n",
       "                                                Text  Retweets  Likes  \\\n",
       "0  Party least receive say or single. Prevent pre...         2     25   \n",
       "1  Hotel still Congress may member staff. Media d...        35     29   \n",
       "2  Nice be her debate industry that year. Film wh...        51     25   \n",
       "3  Laugh explain situation career occur serious. ...        37     18   \n",
       "4  Involve sense former often approach government...        27     80   \n",
       "\n",
       "             Timestamp  \n",
       "0  2023-01-30 11:00:51  \n",
       "1  2023-01-02 22:45:58  \n",
       "2  2023-01-18 11:25:19  \n",
       "3  2023-04-10 22:06:29  \n",
       "4  2023-01-24 07:12:21  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the kaggle dataset we want.\n",
    "path = kagglehub.dataset_download('goyaladi/twitter-dataset')\n",
    "df = pd.read_csv(f\"{path}/twitter_dataset.csv\")\n",
    "df.head()\n",
    "\n",
    "# Define the dataset class. We need a length and getitem method in addition to the initializer.\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.text = df.Text.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # We need to return both the padded token sets and their original lengths\n",
    "        # Get the token IDs, and squeeze to get rid of the batch dimension.\n",
    "        tokens = self.tokenizer(\n",
    "            self.text[idx],\n",
    "            return_tensors='pt',\n",
    "        ).input_ids.squeeze(0)\n",
    "\n",
    "        # Get the lengths, store as tensor to keep consistent with token types.\n",
    "        length = torch.tensor(len(tokens))\n",
    "        \n",
    "        return {\n",
    "            \"tokens\": tokens,\n",
    "            \"length\": length\n",
    "        }\n",
    "\n",
    "# Create the dataset.\n",
    "dataset = TextDataset(df, tokenizer)\n",
    "\n",
    "# Now create a collation function to handle unequal lengths.\n",
    "def collate_fn(batch):\n",
    "    # Pad the tokens\n",
    "    tokens = [item['tokens'] for item in batch]\n",
    "    padded_tokens = nn.utils.rnn.pad_sequence(tokens, batch_first=True)\n",
    "    \n",
    "    # Get the lengths, and store as tensor\n",
    "    lengths = [item['length'] for item in batch]\n",
    "    lengths = torch.stack(lengths)\n",
    "\n",
    "    return {\n",
    "        \"tokens\": padded_tokens,\n",
    "        \"lengths\": lengths\n",
    "    }\n",
    "\n",
    "# Now create the dataloader.\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7d8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1be88d6983498ea4fc1416151b4497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitStuff\\text_vae\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9efb2ec74f407b808d95fd8171c2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d018f41ca4e6d8bb9d3abf7180222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496ab369928f4415986597403d58d725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shining dataset as a test. \"All work and no play makes Jack a dull boy.\"\n",
    "phrase = \"All work and no play makes Jack a dull boy.\"\n",
    "n = 10000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Text': phrase * n\n",
    "})\n",
    "\n",
    "class ShiningDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.length = len(self.df)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenizer(\n",
    "            self.df.iloc[idx]['Text'],\n",
    "            return_tensors = 'pt'\n",
    "        )['input_ids'].squeeze(0)\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'lengths': torch.tensor(self.length)\n",
    "        }\n",
    "    \n",
    "dataset = ShiningDataset(df, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb42c3",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90343a8",
   "metadata": {},
   "source": [
    "The model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Figure out length logic (tokens or characters, how to compute)\n",
    "class TextVAE(nn.Module):\n",
    "    # A VAE that recreates text.\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            hidden_dim,        \n",
    "            num_layers,\n",
    "            dropout,\n",
    "            latent_dim\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the VAE.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        vocab_size: Size of the model vocabulary. Recommend to set equal \n",
    "                    to the number of unique tokens in the training data.\n",
    "        embedding_dim: Dimension of the space in which tokens are embedded.\n",
    "        hidden_dim: Dimension of the hidden layer between the GRU and the\n",
    "                    latent space.\n",
    "        num_layers: Number of layers in each GRU.\n",
    "        dropout: Dropout probability for the GRU.\n",
    "        latent_dim: Dimension of the latent space\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the superclass.\n",
    "        super().__init__()\n",
    "\n",
    "        # Store latent_dim for later use.\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer: Convert tokens into vectors. Used in both encoder and decoder.\n",
    "        # ----\n",
    "        # Input: tensor (batch_size, seq_length)\n",
    "        # Output: tensor (batch_size, seq_length, embedding_dim)\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        # ----\n",
    "\n",
    "        # Encoder GRU: Reads tokens into GRU, a recurrent module which can handle the\n",
    "        # sequence of tokens. Since we're using the hidden states, be mindful to transpose\n",
    "        # things so that the tensor has the right shape afterwards!\n",
    "        # Input: tensor (batch_size, seq_length, embedding_dim)\n",
    "        # Output: output tensor (batch_size, seq_length, hidden_dim)\n",
    "        #         hidden state tensor (num_layers, batch_size, hidden_dim)\n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        # ----\n",
    "\n",
    "        # Fully connected layers: One each for mu and log variance. Note that \n",
    "        # this only uses the hidden state of the last token in the sequence! \n",
    "        # The relevant dimensional shuffling is done in the encoding function. \n",
    "        # In both cases:\n",
    "        # Input: tensor (batch_size, hidden_dim)\n",
    "        # Output: tensor (batch_size, latent_dim)\n",
    "        self.fc_mu = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=latent_dim,\n",
    "        )\n",
    "        self.fc_logvar = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=latent_dim,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        # ----\n",
    "        \n",
    "        # Fully connected layer: Going from the latent space to the decoder's GRU.\n",
    "        # Input: tensor (batch_size, seq_length, latent_dim)\n",
    "        # Output: tensor (batch_size, seq_length, hidden_dim)\n",
    "        self.fc_decoder_init = nn.Linear(\n",
    "            in_features=latent_dim,\n",
    "            out_features=hidden_dim\n",
    "        )\n",
    "\n",
    "        # Decoder GRU: The recurrent part of the decoder. Just like the encoder!\n",
    "        # Input: tensor (batch_size, seq_length, embedding_dim)\n",
    "        # Output: output tensor (batch_size, seq_length, hidden_dim)\n",
    "        #         hidden state tensor (num_layers, batch_size, hidden_dim)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Output layer: Convert GRU hidden states into token logits. Note that \n",
    "        # this only uses the hidden state of the last token in the sequence! \n",
    "        # The relevant dimensional shuffling is done in the decoding function.\n",
    "        # Input: tensor (batch_size, hidden_dim)\n",
    "        # Output: tensor (batch_size, vocab_size)\n",
    "        self.fc_decoder_output = nn.Linear(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "    \n",
    "    def encode(\n",
    "            self, \n",
    "            x, \n",
    "            lengths\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Encode input sequences into a latent distribution, i.e. mu and log variance.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        x: Input tensor of token indices. Shape: (batch_size, seq_length)\n",
    "        lengths: Actual lengths of the sequences before padding. Shape: (batch_size)\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        mu, logvar: Tensors representing the parameters of the latent distribution. Shape: (batch_size, latent_dim) \n",
    "        \"\"\"\n",
    "\n",
    "        # Embed the sequences.\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "\n",
    "        # Pack the sequences for GRU processing. This is a PyTorch data structure that\n",
    "        # helps the GRU ignore padding tokens and process things faster.\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            input=embedded,\n",
    "            lengths=lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # Encode the packed sequence through the GRU.\n",
    "        _, hidden = self.encoder_gru(packed)\n",
    "\n",
    "        # Get the hidden state of the last layer.\n",
    "        last_hidden = hidden[-1:].squeeze(0).contiguous()\n",
    "        last_hidden = last_hidden.view(last_hidden.size(0), -1)\n",
    "\n",
    "        # Get the mean and variance.\n",
    "        mu = self.fc_mu(last_hidden)\n",
    "        logvar = self.fc_logvar(last_hidden)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def latent_sample(\n",
    "            self, \n",
    "            mu, \n",
    "            logvar\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Sample from the latent distribution using the reparameterization trick.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        mu: Mean tensor of the latent distribution. Shape: (batch_size, latent_dim)\n",
    "        logvar: Log variance tensor of the latent distribution. Shape: (batch_size, latent_dim)\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        z: Sampled latent tensor. Shape: (batch_size, latent_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the standard deviation.\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "\n",
    "        # Add Gaussian noise.\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        # Now return the latent vector.\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def decode(\n",
    "            self, \n",
    "            z, \n",
    "            max_length\n",
    "):\n",
    "        \"\"\"\n",
    "        Decode latent vectors into output token logits.\n",
    "        \n",
    "        Inputs:\n",
    "        ----\n",
    "        z: Latent tensor. Shape: (batch_size, latent_dim)\n",
    "        max_length: Maximum length of the output sequences to generate.\n",
    "        \n",
    "        Outputs:\n",
    "        ----\n",
    "        logits: Output token logits: Shape: (batch_size, max_length, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the batch size from the latent tensor.\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        # Get the initial hidden state for the decoder GRU.\n",
    "        decoder_hidden = self.fc_decoder_init(z)\n",
    "        decoder_hidden = decoder_hidden.view(\n",
    "            batch_size, self.hidden_dim\n",
    "        ).contiguous()\n",
    "\n",
    "        # Prepare the input tokens for the decoder GRU. Start with all start of\n",
    "        # sequence tokens.\n",
    "        current_input = torch.full(\n",
    "            size=(batch_size, 1), \n",
    "            fill_value=1, \n",
    "            dtype=torch.long,\n",
    "            device=z.device\n",
    "        )\n",
    "\n",
    "        # Prepare a list to collect the logits.\n",
    "        logits = []\n",
    "\n",
    "        # Produce a logit for each possible max token.\n",
    "        for _ in range(max_length):\n",
    "\n",
    "            # Embed the current inputs.\n",
    "            embedded = self.embedding(current_input)\n",
    "\n",
    "            # Decode the embedding through the GRU. The old value\n",
    "            # of decoder_hidden is used to produce the new one.\n",
    "            output, decoder_hidden = self.decoder_gru(embedded, decoder_hidden)\n",
    "\n",
    "            # Send the output through to get the logits. Squeeze the 1st\n",
    "            # dimension (the seq_length), since we're processing one token\n",
    "            # at a time, and need to match the expected input shape of the \n",
    "            # fully connected layer.\n",
    "            out = self.fc_decoder_output(output.squeeze(1))\n",
    "            logits.append(out)\n",
    "\n",
    "            # Get the next input tokens by grabbing the argmax of the logits.\n",
    "            current_input = torch.argmax(out, dim=-1).unsqueeze(1)\n",
    "\n",
    "        # Finally, stack the logits and return them. Stack vs. cat here, since\n",
    "        # we're adding the sequence length back in as a new dimension.\n",
    "        logits = torch.stack(logits, dim=1)\n",
    "        return logits\n",
    "        \n",
    "    def forward(self, x, lengths, max_length=None):\n",
    "        \"\"\"\n",
    "        The network's forward pass. Returns both the reconstructed logits and\n",
    "        the mu and logvar parameters of the latent distribution. We need both\n",
    "        to calculate the loss.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        x: Input sequences. Shape: (batch_size, seq_length)\n",
    "        lengths: Actual lengths of each sequence.\n",
    "        max_lengths: Maximum length for decoder, defaults to input length\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        logits: Logits of the reconstruted sequences. \n",
    "        Shape: (batch_size, max_length, vocab_size)\n",
    "        mu: Mean vectors of the latent distributions. Shape: (batch_size, \n",
    "        latent_dim).\n",
    "        logvar: Log variance vectors of the latent distributions. Shape: \n",
    "        (batch_size, latent_dim).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set the default max_length.\n",
    "        if max_length is None:\n",
    "            max_length = x.size(1)\n",
    "        \n",
    "        # Encode the input sequences.\n",
    "        mu, logvar = self.encode(x, lengths)\n",
    "\n",
    "        # Sample from the latent distribution.\n",
    "        z = self.latent_sample(mu, logvar)\n",
    "\n",
    "        # Now decode the latent vectors.\n",
    "        logits = self.decode(z, max_length)\n",
    "\n",
    "        return logits, mu, logvar\n",
    "    \n",
    "    def generate(self, num_samples, max_length, mu = None, logvar = None):\n",
    "        \"\"\"\n",
    "        Generate samples from the latent space.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        num_samples: Number of samples to generate.\n",
    "        max_length: Maximum length of each sample, in tokens.\n",
    "        mu: optional mean tensor for the latent distribution. If None, \n",
    "            samples will be drawn from a standard normal distribution.\n",
    "        logvar: optional log variance tensor for the latent distribution.\n",
    "                If None, samples will be drawn from a standard normal\n",
    "                distribution.\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        tokens: Generated samples' token indices. Shape: (num_samples, max_length)\n",
    "        \"\"\"\n",
    "        # Set default mu and logvar values, if necessary.\n",
    "        if mu is None:\n",
    "            mu = torch.zeros((1, self.latent_dim), device=next(self.parameters()).device)\n",
    "        if logvar is None:\n",
    "            logvar = torch.zeros((1, self.latent_dim), device=next(self.parameters()).device)\n",
    "\n",
    "        # Expand mu and logvar to match num_samples. Shape will\n",
    "        # be (num_samples, latent_dim)\n",
    "        mu = mu.expand(num_samples, -1)\n",
    "        logvar = logvar.expand(num_samples, -1)\n",
    "\n",
    "        # Set into eval mode, and disable gradient calculations.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Sample from the distribution.\n",
    "            z = self.latent_sample(mu, logvar)\n",
    "\n",
    "            # Now decode the latent vectors and get the logits. Shape\n",
    "            # is \n",
    "            logits = self.decode(z, max_length)\n",
    "\n",
    "            # Get the token indices by grabbing the largest logits' \n",
    "            # indices. Do across the last dimension, i.e. the vocab\n",
    "            # dimension.\n",
    "            tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758f491",
   "metadata": {},
   "source": [
    "Now the loss function, which incorporates both reconstruction loss and KL-divergence from the prior distribution we want. The loss function is really the evidence lower bound (ELBO) of variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23955a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(\n",
    "    recon_logits,\n",
    "    target_tokens,\n",
    "    mu,\n",
    "    logvar,\n",
    "    prior_mu=None,\n",
    "    prior_logvar=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the VAE's loss, combining reconstruction loss and KL\n",
    "    divergence from a prior distribution.\n",
    "\n",
    "    Inputs:\n",
    "    ----\n",
    "    recon_logits: Reconstructed logits from the VAE. Shape: (batch_size, seq_length, vocab_size)\n",
    "    target_tokens: Target token indices, the ones we want the logits to be\n",
    "                   close to. Shape: (batch_size, seq_length)\n",
    "    mu: Mean vectors of the latent distributions. Shape: (batch_size, latent_dim)\n",
    "    logvar: Log variance vectors of the latent distributions. Shape: (batch_size, latent_dim)\n",
    "    prior_mu: Mean of the prior distribution, default unit normal. Shape: (latent_dim)\n",
    "    prior_logvar: Log variance of the prior distribution, default unit normal.\n",
    "                  Shape: (latent_dim)\n",
    "    \n",
    "    Outputs:\n",
    "    ----\n",
    "    loss: The VAE loss.\n",
    "    recon_loss: The reconstruction part of the loss.\n",
    "    kl_div: The KL divergence part of the loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, get the shape of the reconstructed logits so we can get\n",
    "    # the batch size, sequence length, and vocabulary size.\n",
    "    batch_size, seq_length, vocab_size = recon_logits.shape\n",
    "    latent_dim = mu.shape[1]\n",
    "\n",
    "    # Now set up the prior distribution parameters, if needed.\n",
    "    # TODO: Add logvar\n",
    "    if prior_mu is None:\n",
    "        prior_mu = torch.zeros((1, latent_dim), device=recon_logits.device)\n",
    "    if prior_logvar is None:\n",
    "        prior_logvar = torch.zeros((1, latent_dim), device=recon_logits.device)\n",
    "\n",
    "\n",
    "    # Now we flatten the logits and target tokens. Remember, at this\n",
    "    # point, we don't care about keeping the sequences separate; we\n",
    "    # just need to know how close each logit is to its target token.\n",
    "    logits_flat = recon_logits.view(-1, vocab_size) # One value per possible token, per token.\n",
    "    targets_flat = target_tokens.view(-1) # Just a flat list of target tokens.\n",
    "\n",
    "    # Calculate the reconstruction loss as cross-entropy.\n",
    "    recon_loss = cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "    # Now calculate the KL divergence. This uses a closed-form solution\n",
    "    # for the KL divergence between two Gaussians.\n",
    "    kl_div = 0.5 * torch.sum(\n",
    "        torch.exp(logvar - prior_logvar) + (prior_mu - mu) ** 2\n",
    "        / torch.exp(prior_logvar) - logvar + prior_logvar - 1\n",
    "    )\n",
    "\n",
    "    # Finally, return the completed loss.\n",
    "    loss = recon_loss + kl_div\n",
    "\n",
    "    return loss, recon_loss, kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d67c61",
   "metadata": {},
   "source": [
    "Now define the training loop we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12cbf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(\n",
    "    model,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    prior_mu=None,\n",
    "    prior_logvar=None,\n",
    "    epochs=1,\n",
    "    print_freq=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a VAE on a set of training data.\n",
    "\n",
    "    Inputs:\n",
    "    ----\n",
    "    model: The VAE to be trained.\n",
    "    train_loader: The dataloader for our training dataset.\n",
    "    optimizer: The optimizer we're using.\n",
    "    prior_mu: Mean of the prior distribution. Default unit normal.\n",
    "    prior_logvar: Log variance of the prior distribution. Default unit normal.\n",
    "    epochs: Number of epochs to train for. Default 1.\n",
    "    print_freq: Frequency of printing training progress, in batches. Default 50.\n",
    "\n",
    "    Outputs:\n",
    "    ----\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab the device the model is on.\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Set the prior distribution parameters, if needed. We'll need to expand these later\n",
    "    # to match the batch sizes. We can't do this in advance, since the batches may vary.\n",
    "    if prior_mu is None:\n",
    "        prior_mu = torch.zeros(\n",
    "            size = (1, model.latent_dim),\n",
    "            device = device,\n",
    "        )\n",
    "    if prior_logvar is None:\n",
    "        prior_logvar = torch.zeros(\n",
    "            size = (1, model.latent_dim),\n",
    "            device = device\n",
    "        )\n",
    "\n",
    "    # Guardian statements to ensure prior tensors have correct shape:\n",
    "    assert prior_mu.shape == (1, model.latent_dim), \"prior_mu must have shape (1, latent_dim)\"\n",
    "    assert prior_logvar.shape == (1, model.latent_dim), \"prior_logvar must have shape (1, latent_dim)\"\n",
    "\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over epochs.\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Iterate over batches in the data loader.\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Get the tweets and their lengths.\n",
    "            tokens = batch['tokens']\n",
    "            lengths = batch['lengths']\n",
    "\n",
    "            # Get the batch size.\n",
    "            batch_size = tokens.size(0)\n",
    "\n",
    "            # Zero the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Run the forward pass through the model.\n",
    "            recon_logits, mu, logvar = model(tokens, lengths)\n",
    "\n",
    "            # Calculate the loss, expanding the prior parameters to match the batch size.\n",
    "            loss, recon_loss, kl_div = vae_loss(\n",
    "                recon_logits,\n",
    "                tokens,\n",
    "                mu,\n",
    "                logvar,\n",
    "                prior_mu=prior_mu.expand(batch_size, -1),\n",
    "                prior_logvar=prior_logvar.expand(batch_size, -1)\n",
    "            )\n",
    "\n",
    "            # Backpropagate the loss.\n",
    "            loss.backward()\n",
    "\n",
    "            # Step the optimizer to update the parameters of the model.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print progress when needed.\n",
    "            if (batch_idx+1) % print_freq == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, batch {batch_idx+1}/{len(train_loader)}. Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a00a0c",
   "metadata": {},
   "source": [
    "# The training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32eba7e",
   "metadata": {},
   "source": [
    "The actual training at long last!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8af0401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set important model parameters.\n",
    "vocab_size = tokenizer.vocab_size # Use the tokenizer's vocabulary size.\n",
    "embedding_dim = 128 # Small-ish embedding dimension.\n",
    "hidden_dim = 256 # Small-to-middling hidden dimension for the GRU.\n",
    "num_layers = 1 # Number of GRU layers, set to just one.\n",
    "dropout = 0. # No dropout for now. Probably a bad idea, but let's see how it does.\n",
    "latent_dim = 32 # Dimension of the latent space. Small in general, large compared to what I've seen!\n",
    "lr = 1e-3 # Learning rate, a common value used.\n",
    "\n",
    "# Instantiate the model.\n",
    "textVAE = TextVAE(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    latent_dim=latent_dim\n",
    ")\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = Adam(textVAE.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3db164ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For batched 3-D input, hx should also be 3-D but got 2-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# At long last, train the model!\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtextVAE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mtrain_vae\u001b[39m\u001b[34m(model, train_loader, optimizer, prior_mu, prior_logvar, epochs, print_freq)\u001b[39m\n\u001b[32m     64\u001b[39m optimizer.zero_grad()\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Run the forward pass through the model.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m recon_logits, mu, logvar = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Calculate the loss, expanding the prior parameters to match the batch size.\u001b[39;00m\n\u001b[32m     70\u001b[39m loss, recon_loss, kl_div = vae_loss(\n\u001b[32m     71\u001b[39m     recon_logits,\n\u001b[32m     72\u001b[39m     tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     prior_logvar=prior_logvar.expand(batch_size, -\u001b[32m1\u001b[39m)\n\u001b[32m     77\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\GitStuff\\text_vae\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\GitStuff\\text_vae\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 286\u001b[39m, in \u001b[36mTextVAE.forward\u001b[39m\u001b[34m(self, x, lengths, max_length)\u001b[39m\n\u001b[32m    283\u001b[39m z = \u001b[38;5;28mself\u001b[39m.latent_sample(mu, logvar)\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# Now decode the latent vectors.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits, mu, logvar\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 236\u001b[39m, in \u001b[36mTextVAE.decode\u001b[39m\u001b[34m(self, z, max_length)\u001b[39m\n\u001b[32m    232\u001b[39m embedded = \u001b[38;5;28mself\u001b[39m.embedding(current_input)\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# Decode the embedding through the GRU. The old value\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;66;03m# of decoder_hidden is used to produce the new one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m output, decoder_hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder_gru\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# Send the output through to get the logits. Squeeze the 1st\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# dimension (the seq_length), since we're processing one token\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# at a time, and need to match the expected input shape of the \u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# fully connected layer.\u001b[39;00m\n\u001b[32m    242\u001b[39m out = \u001b[38;5;28mself\u001b[39m.fc_decoder_output(output.squeeze(\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\GitStuff\\text_vae\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\GitStuff\\text_vae\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documents\\GitStuff\\text_vae\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1372\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hx.dim() != \u001b[32m3\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1373\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFor batched 3-D input, hx should also be 3-D but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-D tensor\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1374\u001b[39m         )\n\u001b[32m   1375\u001b[39m max_batch_size = \u001b[38;5;28minput\u001b[39m.size(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28minput\u001b[39m.size(\u001b[32m1\u001b[39m)\n\u001b[32m   1376\u001b[39m sorted_indices = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: For batched 3-D input, hx should also be 3-D but got 2-D tensor"
     ]
    }
   ],
   "source": [
    "# At long last, train the model!\n",
    "train_vae(\n",
    "    model=textVAE,\n",
    "    train_loader=dataloader,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93acec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
