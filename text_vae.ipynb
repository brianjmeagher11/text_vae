{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6690cc8",
   "metadata": {},
   "source": [
    "# VAE for text generation\n",
    "\n",
    "Just a little experiment I'm running to see if I can do it. I have an agentic version of this repo too, but want to make sure I can code it from scratch as well.\n",
    "\n",
    "My goal is to get a network that can generate synthetic tweets after being trained on an appropriately sized dataset... or at least could, in principle. I'll need to learn more about how to set up recurrent architectures than I currently know, so this is a good exercise for me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fded35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVAE(nn.Module):\n",
    "    # A VAE that recreates text. Specifically, I'll use it for tweets.\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            hidden_dim,        \n",
    "            num_layers,\n",
    "            dropout,\n",
    "            latent_dim\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the VAE.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        vocab_size: Size of the model vocabulary. Recommend to set equal \n",
    "                    to the number of unique tokens in the training data.\n",
    "        embedding_dim: Dimension of the space in which tokens are embedded.\n",
    "        hidden_dim: Dimension of the hidden layer between the GRU and the\n",
    "                    latent space.\n",
    "        num_layers: Number of layers in each GRU.\n",
    "        dropout: Dropout probability for the GRU.\n",
    "        latent_dim: Dimension of the latent space\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the superclass.\n",
    "        super().__init__()\n",
    "\n",
    "        # Store latent_dim for later use.\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Embedding layer: Convert tokens into vectors. Used in both encoder and decoder.\n",
    "        # ----\n",
    "        # Input: tensor (batch_size, seq_length)\n",
    "        # Output: tensor (batch_size, seq_length, embedding_dim)\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        # ----\n",
    "\n",
    "        # Encoder GRU: Reads tokens into GRU, a recurrent module which can handle the\n",
    "        # sequence of tokens. Since we're using the hidden states, be mindful to transpose\n",
    "        # things so that the tensor has the right shape afterwards!\n",
    "        # Input: tensor (batch_size, seq_length, embedding_dim)\n",
    "        # Output: output tensor (batch_size, seq_length, hidden_dim)\n",
    "        #         hidden state tensor (num_layers, batch_size, hidden_dim)\n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        # ----\n",
    "\n",
    "        # Fully connected layers: One each for mu and log variance. Note that \n",
    "        # this only uses the hidden state of the last token in the sequence! \n",
    "        # The relevant dimensional shuffling is done in the encoding function. \n",
    "        # In both cases:\n",
    "        # Input: tensor (batch_size, hidden_dim)\n",
    "        # Output: tensor (batch_size, latent_dim)\n",
    "        self.fc_mu = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=latent_dim,\n",
    "        )\n",
    "        self.fc_logvar = nn.Linear(\n",
    "            in_features=hidden_dim, \n",
    "            out_features=latent_dim,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        # ----\n",
    "        \n",
    "        # Fully connected layer: Going from the latent space to the decoder's GRU.\n",
    "        # Input: tensor (batch_size, seq_length, latent_dim)\n",
    "        # Output: tensor (batch_size, seq_length, hidden_dim)\n",
    "        self.fc_decoder_init = nn.Linear(\n",
    "            in_features=latent_dim,\n",
    "            out_features=hidden_dim\n",
    "        )\n",
    "\n",
    "        # # Decoder embedding layer: Convert max hidden state indices (i.e. tokens) into vectors\n",
    "        # # Input: tensor (batch_size, seq_length)\n",
    "        # # Output: tensor (batch_size, seq_length, embedding_dim)\n",
    "        # self.decoder_embedding = nn.Embedding(\n",
    "        #     vocab_size,\n",
    "        #     embedding_dim,\n",
    "        #     padding_idx=0,\n",
    "        # )\n",
    "\n",
    "        # Decoder GRU: The recurrent part of the decoder. Just like the encoder!\n",
    "        # Input: tensor (batch_size, seq_length, embedding_dim)\n",
    "        # Output: output tensor (batch_size, seq_length, hidden_dim)\n",
    "        #         hidden state tensor (num_layers, batch_size, hidden_dim)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Output layer: Convert GRU hidden states into token logits. Note that \n",
    "        # this only uses the hidden state of the last token in the sequence! \n",
    "        # The relevant dimensional shuffling is done in the decoding function.\n",
    "        # Input: tensor (batch_size, hidden_dim)\n",
    "        # Output: tensor (batch_size, vocab_size)\n",
    "        self.fc_decoder_output = nn.Linear(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=vocab_size\n",
    "        )\n",
    "    \n",
    "    def encode(\n",
    "            self, \n",
    "            x, \n",
    "            lengths\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Encode input sequences into a latent distribution, i.e. mu and log variance.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        x: Input tensor of token indices. Shape: (batch_size, seq_length)\n",
    "        lengths: Actual lengths of the sequences before padding. Shape: (batch_size)\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        Mu, logvar: Tensors representing the parameters of the latent distribution. Shape: (batch_size, latent_dim) \n",
    "        \"\"\"\n",
    "\n",
    "        # Embed the sequences.\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Pack the sequences for GRU processing. This is a PyTorch data structure that\n",
    "        # helps the GRU ignore padding tokens and process things faster.\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            input=embedded,\n",
    "            lengths=lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # Encode the packed sequence through the GRU.\n",
    "        _, hidden = self.encoder_gru(packed)\n",
    "\n",
    "        # Get the hidden state of the last layer.\n",
    "        last_hidden = hidden[-1:].squeeze(0).contiguous()\n",
    "        last_hidden = last_hidden.view(last_hidden.size(0), -1)\n",
    "\n",
    "        # Get the mean and variance.\n",
    "        mu = self.fc_mu(last_hidden)\n",
    "        logvar = self.fc_logvar(last_hidden)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def latent_sample(\n",
    "            self, \n",
    "            mu, \n",
    "            logvar\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Sample from the latent distribution using the reparameterization trick.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        mu: Mean tensor of the latent distribution. Shape: (batch_size, latent_dim)\n",
    "        logvar: Log variance tensor of the latent distribution. Shape: (batch_size, latent_dim)\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        z: Sampled latent tensor. Shape: (batch_size, latent_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the standard deviation.\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "\n",
    "        # Add Gaussian noise.\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        # Now return the latent vector.\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def decode(\n",
    "            self, \n",
    "            z, \n",
    "            max_length\n",
    "):\n",
    "        \"\"\"\n",
    "        Decode latent vectors into output token logits.\n",
    "        \n",
    "        Inputs:\n",
    "        ----\n",
    "        z: Latent tensor. Shape: (batch_size, latent_dim)\n",
    "        max_length: Maximum length of the output sequences to generate.\n",
    "        \n",
    "        Outputs:\n",
    "        ----\n",
    "        logits: Output token logits: Shape: (batch_size, max_length, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the batch size from the latent tensor.\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        # Get the initial hidden state for the decoder GRU.\n",
    "        decoder_hidden = self.fc_decoder_init(z)\n",
    "        decoder_hidden = decoder_hidden.view(\n",
    "            batch_size, self.hidden_dim\n",
    "        ).contiguous()\n",
    "\n",
    "        # Prepare the input tokens for the decoder GRU. Start with all start of\n",
    "        # sequence tokens.\n",
    "        current_input = torch.full(\n",
    "            size=(batch_size, 1), \n",
    "            fill_value=1, \n",
    "            dtype=torch.long,\n",
    "            device=z.device\n",
    "        )\n",
    "\n",
    "        # Prepare a list to collect the logits.\n",
    "        logits = []\n",
    "\n",
    "        # Produce a logit for each possible max token.\n",
    "        for _ in range(max_length):\n",
    "\n",
    "            # Embed the current inputs.\n",
    "            embedded = self.embedding(current_input)\n",
    "\n",
    "            # Decode the embedding through the GRU. The old value\n",
    "            # of decoder_hidden is used to produce the new one.\n",
    "            output, decoder_hidden = self.decoder_gru(embedded, decoder_hidden)\n",
    "\n",
    "            # Send the output through to get the logits. Squeeze the 1st\n",
    "            # dimension (the seq_length), since we're processing one token\n",
    "            # at a time, and need to match the expected input shape of the \n",
    "            # fully connected layer.\n",
    "            out = self.fc_decoder_output(output.squeeze(1))\n",
    "            logits.append(out)\n",
    "\n",
    "        # Finally, stack the logits and return them. Stack vs. cat here, since\n",
    "        # we're adding the sequence length back in as a new dimension.\n",
    "        logits = torch.stack(logits, dim=1)\n",
    "        return logits\n",
    "        \n",
    "    def forward(self, x, lengths, max_length=None):\n",
    "        \"\"\"\n",
    "        The network's forward pass. Returns both the reconstructed logits and\n",
    "        the mu and logvar parameters of the latent distribution. We need both\n",
    "        to calculate the loss.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        x: Input sequences. Shape: (batch_size, seq_length)\n",
    "        lengths: Actual lengths of each sequence.\n",
    "        max_lengths: Maximum length for decoder, defaults to input length\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        logits: Logits of the reconstruted sequences. \n",
    "        Shape: (batch_size, max_length, vocab_size)\n",
    "        mu: Mean vectors of the latent distributions. Shape: (batch_size, \n",
    "        latent_dim).\n",
    "        logvar: Log variance vectors of the latent distributions. Shape: \n",
    "        (batch_size, latent_dim).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set the default max_length.\n",
    "        if max_length is None:\n",
    "            max_length = x.size(1)\n",
    "        \n",
    "        # Encode the input sequences.\n",
    "        mu, logvar = self.encode(x, lengths)\n",
    "\n",
    "        # Sample from the latent distribution.\n",
    "        z = self.latent_sample(mu, logvar)\n",
    "\n",
    "        # Now decode the latent vectors.\n",
    "        logits = self.decode(z, max_length)\n",
    "\n",
    "        return logits, mu, logvar\n",
    "    \n",
    "    def generate(self, num_samples, max_length, mu = None, logvar = None):\n",
    "        \"\"\"\n",
    "        Generate samples from the latent space.\n",
    "\n",
    "        Inputs:\n",
    "        ----\n",
    "        num_samples: Number of samples to generate.\n",
    "        max_length: Maximum length of each sample, in tokens.\n",
    "        mu: optional mean tensor for the latent distribution. If None, \n",
    "            samples will be drawn from a standard normal distribution.\n",
    "        logvar: optional log variance tensor for the latent distribution.\n",
    "                If None, samples will be drawn from a standard normal\n",
    "                distribution.\n",
    "\n",
    "        Outputs:\n",
    "        ----\n",
    "        tokens: Generated samples' token indices. Shape: (num_samples, max_length)\n",
    "        \"\"\"\n",
    "        # Set default mu and logvar values, if necessary.\n",
    "        if mu is None:\n",
    "            mu = torch.zeros((1, self.latent_dim), device=next(self.parameters()).device)\n",
    "        if logvar is None:\n",
    "            logvar = torch.zeros((1, self.latent_dim), device=next(self.parameters()).device)\n",
    "\n",
    "        # Expand mu and logvar to match num_samples. Shape will\n",
    "        # be (num_samples, latent_dim)\n",
    "        mu = mu.expand(num_samples, -1)\n",
    "        logvar = logvar.expand(num_samples, -1)\n",
    "\n",
    "        # Set into eval mode, and disable gradient calculations.\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Sample from the distribution.\n",
    "            z = self.latent_sample(mu, logvar)\n",
    "\n",
    "            # Now decode the latent vectors and get the logits. Shape\n",
    "            # is \n",
    "            logits = self.decode(z, max_length)\n",
    "\n",
    "            # Get the token indices by grabbing the largest logits' \n",
    "            # indices. Do across the last dimension, i.e. the vocab\n",
    "            # dimension.\n",
    "            tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        return tokens"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
